{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 2048, 3)\n",
      "(10240, 1)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "files = ['../Data/ModelNet40_train/ply_data_train0.h5',\n",
    "         '../Data/ModelNet40_train/ply_data_train1.h5',\n",
    "         '../Data/ModelNet40_train/ply_data_train2.h5',\n",
    "         '../Data/ModelNet40_train/ply_data_train3.h5',\n",
    "         '../Data/ModelNet40_train/ply_data_train4.h5']\n",
    "#files = ['../Data/ModelNet10_train/modelnet10_train.h5']\n",
    "d = []\n",
    "l = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    fh5 = h5py.File(files[0], 'r')\n",
    "    data = fh5['data'][:]\n",
    "    label = fh5['label'][:]\n",
    "    fh5.close()\n",
    "    if(i != 0):\n",
    "        d = np.append(d, data, axis=0)\n",
    "        l = np.append(l, label, axis=0)\n",
    "    else:\n",
    "        d = data\n",
    "        l = label\n",
    "\n",
    "print d.shape\n",
    "print l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEBdJREFUeJzt3WusXFd5xvH/09y4CudyZLm2U4cSgRBqIToNQUEIJS3N\nBeFUCigVAhe5slRBC6UVMa3U0JsUqpYAUpXKJRBDKZAGqlgQlbpJEOoHAjYJuUJzCAm25cSGXKBF\nFFLefpjlMBhfkpnjmTln/X/S0ey99p7Z71n2zDNr7T1zUlVIkvrzC9MuQJI0HQaAJHXKAJCkThkA\nktQpA0CSOmUASFKnjhoAST6cZF+Su4baTkmyPcl97fbk1p4kH0yykOSOJGcN3WdD2/++JBuOza8j\nSXqqnsoI4FrggoPaNgM3VdWZwE1tHeBC4Mz2swm4GgaBAVwBvBw4G7jiQGhIkqbjqAFQVV8EHjmo\neT2wtS1vBS4Zav9oDXwJWJFkFfCbwPaqeqSqHgW28/OhIkmaoONHvN/Kqtrblh8CVrbl1cCuof12\nt7bDtR/RaaedVuvWrRuxREnq086dO79TVXNH22/UAHhSVVWSRfs+iSSbGEwfcfrpp7Njx47FemhJ\n6kKSB5/KfqNeBfRwm9qh3e5r7XuAtUP7rWlth2v/OVW1parmq2p+bu6oASZJGtGoAbANOHAlzwbg\nhqH2N7ergc4BHm9TRZ8HXpPk5Hby9zWtTZI0JUedAkryCeDVwGlJdjO4mudK4LokG4EHgTe03W8E\nLgIWgB8AbwGoqkeS/CXwlbbfX1TVwSeWJUkTlFn+Ouj5+fnyHIAkPT1JdlbV/NH285PAktQpA0CS\nOmUASFKnDABJ6pQBIEmdGvuTwJod6zZ/7snlB668eIqVSFoKHAFIUqcMAEnqlAEgSZ0yACSpUwaA\nJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhS\npwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVq\nrABI8odJ7k5yV5JPJHlGkjOS3JpkIcmnkpzY9j2prS+07esW4xeQJI1m5ABIshr4A2C+ql4CHAdc\nBrwXuKqqXgA8Cmxsd9kIPNrar2r7SZKmZNwpoOOBZyY5HngWsBc4D7i+bd8KXNKW17d12vbzk2TM\n40uSRjRyAFTVHuBvgW8zeOF/HNgJPFZVT7TddgOr2/JqYFe77xNt/1MPftwkm5LsSLJj//79o5Yn\nSTqKcaaATmbwrv4M4BeBZwMXjFtQVW2pqvmqmp+bmxv34SRJhzHOFNCvA9+qqv1V9WPgM8C5wIo2\nJQSwBtjTlvcAawHa9ucB3x3j+JKkMYwTAN8GzknyrDaXfz5wD3ALcGnbZwNwQ1ve1tZp22+uqhrj\n+JKkMYxzDuBWBidzvwrc2R5rC3A58M4kCwzm+K9pd7kGOLW1vxPYPEbdkqQxHX/0XQ6vqq4Arjio\n+X7g7EPs+0Pg9eMcT5K0ePwksCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAk\ndcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTo31JyFn3brNn3ty+YEr\nL55iJZKG+dycDY4AJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXK\nAJCkThkAktQpA0CSOmUASFKnDABJ6tRYAZBkRZLrk3w9yb1JXpHklCTbk9zXbk9u+ybJB5MsJLkj\nyVmL8ytIkkYx7gjgA8C/VdWLgF8F7gU2AzdV1ZnATW0d4ELgzPazCbh6zGNLksYwcgAkeR7wKuAa\ngKr6UVU9BqwHtrbdtgKXtOX1wEdr4EvAiiSrRq5ckjSWcUYAZwD7gY8kuS3Jh5I8G1hZVXvbPg8B\nK9vyamDX0P13tzZJ0hSMEwDHA2cBV1fVy4D/4afTPQBUVQH1dB40yaYkO5Ls2L9//xjlSZKOZJwA\n2A3srqpb2/r1DALh4QNTO+12X9u+B1g7dP81re1nVNWWqpqvqvm5ubkxypMkHcnIAVBVDwG7kryw\nNZ0P3ANsAza0tg3ADW15G/DmdjXQOcDjQ1NFkqQJO37M+/8+8PEkJwL3A29hECrXJdkIPAi8oe17\nI3ARsAD8oO0rSZqSsQKgqm4H5g+x6fxD7FvAW8c5niRp8fhJYEnqlAEgSZ0yACSpUwaAJHXKAJCk\nThkAktSpcT8HIGlM6zZ/7snlB668eIqVqDeOACSpUwaAJHXKAJCkTnkO4BhwTlfSUuAIQJI6ZQBI\nUqcMAEnqlAEgSZ0yACSpUwaAJHXKy0AlaREtpcvAHQFIUqcMAEnqlAEgSZ0yACSpUwaAJHXKq4AO\nYymdyZekUTgCkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpsQMg\nyXFJbkvy2bZ+RpJbkywk+VSSE1v7SW19oW1fN+6xJUmjW4wRwNuBe4fW3wtcVVUvAB4FNrb2jcCj\nrf2qtp8kaUrGCoAka4CLgQ+19QDnAde3XbYCl7Tl9W2dtv38tr8kaQrGHQG8H3gX8JO2firwWFU9\n0dZ3A6vb8mpgF0Db/njbX5I0BSMHQJLXAvuqauci1kOSTUl2JNmxf//+xXxoSdKQcUYA5wKvS/IA\n8EkGUz8fAFYkOfB3BtYAe9ryHmAtQNv+POC7Bz9oVW2pqvmqmp+bmxujPEnSkYwcAFX17qpaU1Xr\ngMuAm6vqjcAtwKVttw3ADW15W1unbb+5qmrU40uSxnMs/iLY5cAnk/wVcBtwTWu/BvhYkgXgEQah\nIUmLzr/o99QsSgBU1ReAL7Tl+4GzD7HPD4HXL8bxJEnj85PAktQpA0CSOmUASFKnDABJ6pQBIEmd\nMgAkqVMGgCR16lh8EEySlqyePkTmCECSOmUASFKnDABJ6pQBIEmd8iSwJI1gOZwsdgQgSZ0yACSp\nU04BTdByGDJKWj4MAKlTviGRU0CS1CkDQJI6ZQBIUqcMAEnqlAEgSZ3yKiDpGPJKG80yRwCS1CkD\nQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKn/C6gJcLvlOmP/+Y6\n1kYeASRZm+SWJPckuTvJ21v7KUm2J7mv3Z7c2pPkg0kWktyR5KzF+iUkSU/fOCOAJ4A/qqqvJnku\nsDPJduB3gJuq6sokm4HNwOXAhcCZ7eflwNXtVp3zna40HSMHQFXtBfa25e8nuRdYDawHXt122wp8\ngUEArAc+WlUFfCnJiiSr2uMsGb5YaalZqv9nl2rdS8minANIsg54GXArsHLoRf0hYGVbXg3sGrrb\n7tb2MwGQZBOwCeD0009fjPI0AT5ZpaVn7KuAkjwH+DTwjqr63vC29m6/ns7jVdWWqpqvqvm5ublx\ny5MkHcZYAZDkBAYv/h+vqs+05oeTrGrbVwH7WvseYO3Q3de0NknSFIxzFVCAa4B7q+p9Q5u2ARva\n8gbghqH2N7ergc4BHl9q8/+StJyMcw7gXOBNwJ1Jbm9tfwJcCVyXZCPwIPCGtu1G4CJgAfgB8JYx\nji1JGtM4VwH9J5DDbD7/EPsX8NZRjycdjSeipafHTwJrSfDFXVp8fheQJHXKEYCWPUcP0qEZAHpa\nlvOL6XL+3aRDcQpIkjplAEhSp5wCkhaB00daigyADvliJfk8AKeAJKlbjgCko/CdopYrRwCS1CkD\nQJI65RSQpJE4NTa6Wek7A0DSEY3zYnWsX+hm5YV0qXIKSJI65QhAWsJ8B6xxOAKQpE51OQLwXZMk\nOQKQpG4ZAJLUqS6ngDQdTr1JRzbp54gjAEnqlAEgSZ1yCmjGOE0iaVIcAUhSpwwASeqUASBJnTIA\nJKlTBoAkdcqrgPQkr0CS+uIIQJI6ZQBIUqcMAEnqlAEgSZ2aeAAkuSDJN5IsJNk86eNLkgYmGgBJ\njgP+HrgQeDHw20lePMkaJEkDkx4BnA0sVNX9VfUj4JPA+gnXIEli8gGwGtg1tL67tUmSJixVNbmD\nJZcCF1TV77b1NwEvr6q3De2zCdjUVl8IfGOMQ54GfGeM+x9L1jYaaxuNtY1mqdb2S1U1d7QHmPQn\ngfcAa4fW17S2J1XVFmDLYhwsyY6qml+Mx1ps1jYaaxuNtY1mudc26SmgrwBnJjkjyYnAZcC2Cdcg\nSWLCI4CqeiLJ24DPA8cBH66quydZgyRpYOJfBldVNwI3TuhwizKVdIxY22isbTTWNpplXdtETwJL\nkmaHXwUhSZ1algEwy183keSBJHcmuT3JjinX8uEk+5LcNdR2SpLtSe5rtyfPUG3vSbKn9d3tSS6a\nUm1rk9yS5J4kdyd5e2ufet8dobap912SZyT5cpKvtdr+vLWfkeTW9nz9VLtAZFZquzbJt4b67aWT\nrm2oxuOS3Jbks219/H6rqmX1w+Dk8jeB5wMnAl8DXjztuobqewA4bdp1tFpeBZwF3DXU9jfA5ra8\nGXjvDNX2HuCPZ6DfVgFnteXnAv/F4KtNpt53R6ht6n0HBHhOWz4BuBU4B7gOuKy1/wPwezNU27XA\npdP+P9fqeifwz8Bn2/rY/bYcRwB+3cRTVFVfBB45qHk9sLUtbwUumWhRzWFqmwlVtbeqvtqWvw/c\ny+AT7VPvuyPUNnU18N9t9YT2U8B5wPWtfVr9drjaZkKSNcDFwIfaeliEfluOATDrXzdRwL8n2dk+\n9TxrVlbV3rb8ELBymsUcwtuS3NGmiKYyPTUsyTrgZQzeMc5U3x1UG8xA37VpjNuBfcB2BqP1x6rq\nibbL1J6vB9dWVQf67a9bv12V5KRp1Aa8H3gX8JO2fiqL0G/LMQBm3Sur6iwG34j61iSvmnZBh1OD\nseXMvAsCrgZ+GXgpsBf4u2kWk+Q5wKeBd1TV94a3TbvvDlHbTPRdVf1fVb2UwbcAnA28aBp1HMrB\ntSV5CfBuBjX+GnAKcPmk60ryWmBfVe1c7MdejgFw1K+bmKaq2tNu9wH/yuBJMEseTrIKoN3um3I9\nT6qqh9uT9CfAPzLFvktyAoMX2I9X1Wda80z03aFqm6W+a/U8BtwCvAJYkeTAZ5Km/nwdqu2CNqVW\nVfW/wEeYTr+dC7wuyQMMprTPAz7AIvTbcgyAmf26iSTPTvLcA8vAa4C7jnyvidsGbGjLG4AbpljL\nzzjw4tr8FlPquzb/eg1wb1W9b2jT1PvucLXNQt8lmUuyoi0/E/gNBucobgEubbtNq98OVdvXhwI9\nDObYJ95vVfXuqlpTVesYvJ7dXFVvZDH6bdpnto/R2fKLGFz98E3gT6ddz1Bdz2dwVdLXgLunXRvw\nCQbTAT9mMIe4kcHc4k3AfcB/AKfMUG0fA+4E7mDwYrtqSrW9ksH0zh3A7e3nolnouyPUNvW+A34F\nuK3VcBfwZ639+cCXgQXgX4CTZqi2m1u/3QX8E+1KoWn9AK/mp1cBjd1vfhJYkjq1HKeAJElPgQEg\nSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKn/h9OGhliY1kWdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb82c4d6fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(l, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 40)\n",
      "Loaded dataset with 40 classes\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(l)\n",
    "classes = Y_train.shape[1]\n",
    "print Y_train.shape\n",
    "print \"Loaded dataset with %s classes\"%(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "# now we need to voxelize that point cloud...\n",
    "def voxelize(dim, data):\n",
    "    # uncomment below if you have not already normalized your object to [0,1]^3\n",
    "    #m = max(x.min(), x.max(), key=abs)\n",
    "    #data /= m # This puts the data in [0,1]\n",
    "    data *= (dim/2) # This puts the data in [0,dim]\n",
    "    data += (dim/2) \n",
    "    data = np.asarray([[int(i[0]), int(i[1]), int(i[2])] for i in data])\n",
    "    data = np.unique(data, axis=1)\n",
    "    retval = np.zeros((dim, dim, dim))\n",
    "    for i in data:\n",
    "        retval[i[0]][i[1]][i[2]] = 1\n",
    "    retval = np.asarray([retval])\n",
    "    return retval\n",
    "\n",
    "X_train = [voxelize(32, i) for i in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 32, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "X_train = np.reshape(X_train, (-1, 32, 32, 32, 1))\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 2048, 3)\n",
      "(4096, 1)\n"
     ]
    }
   ],
   "source": [
    "files = ['../Data/ModelNet40_test/ply_data_test0.h5',\n",
    "              '../Data/ModelNet40_test/ply_data_test1.h5']\n",
    "\n",
    "d = []\n",
    "l = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    fh5 = h5py.File(files[0], 'r')\n",
    "    data = fh5['data'][:]\n",
    "    label = fh5['label'][:]\n",
    "    fh5.close()\n",
    "    if(i != 0):\n",
    "        d = np.append(d, data, axis=0)\n",
    "        l = np.append(l, label, axis=0)\n",
    "    else:\n",
    "        d = data\n",
    "        l = label\n",
    "\n",
    "print d.shape\n",
    "print l.shape\n",
    "\n",
    "Y_test = to_categorical(l)\n",
    "X_test = [voxelize(32, i) for i in d]\n",
    "X_test = np.asarray(X_test)\n",
    "X_test = np.reshape(X_test, (-1, 32, 32, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 14, 14, 14, 32)    4032      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 14, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 14, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 12, 12, 12, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 6, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               884864    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                5160      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 40)                0         \n",
      "=================================================================\n",
      "Total params: 921,736\n",
      "Trainable params: 921,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/activations.py:103: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  ).format(identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution3D, MaxPooling3D\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "num_classes = classes\n",
    "\n",
    "# Defining VoxNet in Keras 2\n",
    "model = Sequential()\n",
    "model.add(Conv3D(input_shape=(32, 32, 32, 1), filters=32, \n",
    "                 kernel_size=(5,5,5), strides=(2, 2, 2)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.1)))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Conv3D(filters=32, kernel_size=(3,3,3)))\n",
    "model.add(Activation(LeakyReLU(alpha=0.1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=None))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(units=num_classes, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10240 samples, validate on 4096 samples\n",
      "Epoch 1/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.2134 - acc: 0.9272 - val_loss: 1.9852 - val_acc: 0.6299\n",
      "Epoch 2/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1999 - acc: 0.9334 - val_loss: 1.9680 - val_acc: 0.6299\n",
      "Epoch 3/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1952 - acc: 0.9311 - val_loss: 2.0854 - val_acc: 0.6343\n",
      "Epoch 4/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1826 - acc: 0.9374 - val_loss: 2.0867 - val_acc: 0.6377\n",
      "Epoch 5/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1834 - acc: 0.9357 - val_loss: 2.1404 - val_acc: 0.6294\n",
      "Epoch 6/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1698 - acc: 0.9421 - val_loss: 2.1168 - val_acc: 0.6392\n",
      "Epoch 7/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1543 - acc: 0.9451 - val_loss: 2.1695 - val_acc: 0.6357\n",
      "Epoch 8/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1586 - acc: 0.9460 - val_loss: 2.1634 - val_acc: 0.6348\n",
      "Epoch 9/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1500 - acc: 0.9474 - val_loss: 2.1505 - val_acc: 0.6440\n",
      "Epoch 10/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1389 - acc: 0.9530 - val_loss: 2.1863 - val_acc: 0.6396\n",
      "Epoch 11/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1360 - acc: 0.9529 - val_loss: 2.2220 - val_acc: 0.6304\n",
      "Epoch 12/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1304 - acc: 0.9522 - val_loss: 2.1790 - val_acc: 0.6450\n",
      "Epoch 13/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1224 - acc: 0.9565 - val_loss: 2.2488 - val_acc: 0.6372\n",
      "Epoch 14/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1149 - acc: 0.9598 - val_loss: 2.2520 - val_acc: 0.6382\n",
      "Epoch 15/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1160 - acc: 0.9604 - val_loss: 2.3152 - val_acc: 0.6445\n",
      "Epoch 16/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1134 - acc: 0.9600 - val_loss: 2.3379 - val_acc: 0.6372\n",
      "Epoch 17/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1132 - acc: 0.9615 - val_loss: 2.3237 - val_acc: 0.6450\n",
      "Epoch 18/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1027 - acc: 0.9672 - val_loss: 2.3943 - val_acc: 0.6323\n",
      "Epoch 19/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.0970 - acc: 0.9653 - val_loss: 2.4038 - val_acc: 0.6377\n",
      "Epoch 20/25\n",
      "10240/10240 [==============================] - 5s - loss: 0.1048 - acc: 0.9608 - val_loss: 2.3850 - val_acc: 0.6338\n",
      "Epoch 21/25\n",
      " 2528/10240 [======>.......................] - ETA: 3s - loss: 0.0887 - acc: 0.9719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2bdeaa732f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(x=X_train, y=Y_train, batch_size=16, \n\u001b[0;32m----> 2\u001b[0;31m                     epochs=25, verbose=1, validation_data=(X_test, Y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y=Y_train, batch_size=16, \n",
    "                    epochs=25, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "#model_json = model.to_json()\n",
    "#with open(\"voxnet40.json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"VoxNet-ModelNet40.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
